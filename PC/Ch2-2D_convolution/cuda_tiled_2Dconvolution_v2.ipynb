{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T07:22:12.253076Z",
     "iopub.status.busy": "2024-12-10T07:22:12.252720Z",
     "iopub.status.idle": "2024-12-10T07:22:13.305687Z",
     "shell.execute_reply": "2024-12-10T07:22:13.304636Z",
     "shell.execute_reply.started": "2024-12-10T07:22:12.253047Z"
    },
    "id": "nNS2FA5BR0bU",
    "outputId": "bd3b83c4-09e3-4336-d9ba-7c2f16e59a9c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Wed_Nov_22_10:17:15_PST_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.107\n",
      "Build cuda_12.3.r12.3/compiler.33567101_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6hCYQF3T2f7"
   },
   "source": [
    "## Initialize a nvcc plugin for python notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T07:22:13.308992Z",
     "iopub.status.busy": "2024-12-10T07:22:13.308292Z",
     "iopub.status.idle": "2024-12-10T07:22:22.753306Z",
     "shell.execute_reply": "2024-12-10T07:22:22.752449Z",
     "shell.execute_reply.started": "2024-12-10T07:22:13.308949Z"
    },
    "id": "e1MqBxDxUBTo",
    "outputId": "1aa2e545-1665-46d8-bd82-aa928d917ff2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvcc4jupyter\n",
      "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: nvcc4jupyter\n",
      "Successfully installed nvcc4jupyter-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nvcc4jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlUTHXk-UM0o"
   },
   "source": [
    "## Load the plugin extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T07:22:22.754713Z",
     "iopub.status.busy": "2024-12-10T07:22:22.754433Z",
     "iopub.status.idle": "2024-12-10T07:24:54.940111Z",
     "shell.execute_reply": "2024-12-10T07:24:54.939195Z",
     "shell.execute_reply.started": "2024-12-10T07:22:22.754687Z"
    },
    "id": "V23O5ZJFUQn4",
    "outputId": "824c4fc6-02fa-4e6f-f07f-62cada73033b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected platform \"Kaggle\". Running its setup...\n",
      "Updating the package lists...\n",
      "Installing nvidia-cuda-toolkit, this may take a few minutes...\n",
      "Source files will be saved in \"/tmp/tmpfghygz6i\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6PwQnXhCBnH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-10T09:54:03.089695Z",
     "iopub.status.busy": "2024-12-10T09:54:03.089356Z",
     "iopub.status.idle": "2024-12-10T09:54:09.458414Z",
     "shell.execute_reply": "2024-12-10T09:54:09.457471Z",
     "shell.execute_reply.started": "2024-12-10T09:54:03.089665Z"
    },
    "id": "vfHv2_VUCBXH",
    "outputId": "cd7f6766-779d-49d6-e7ef-93a41dc5d02c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Number: 0\n",
      "  Device name: Tesla T4\n",
      "  max Blocks Per MultiProcessor: 16\n",
      "  max Threads Per MultiProcessor: 1024\n",
      "  max Threads Per Block: 1024\n",
      "  num SM: 40\n",
      "  num bytes sharedMem Per Block: 49152\n",
      "  num bytes sharedMem Per Multiprocessor: 65536\n",
      "  Memory Clock Rate (KHz): 5001000\n",
      "  Memory Bus Width (bits): 256\n",
      "  Peak Memory Bandwidth (GB/s): 320.064000\n",
      "\n",
      "Device Number: 1\n",
      "  Device name: Tesla T4\n",
      "  max Blocks Per MultiProcessor: 16\n",
      "  max Threads Per MultiProcessor: 1024\n",
      "  max Threads Per Block: 1024\n",
      "  num SM: 40\n",
      "  num bytes sharedMem Per Block: 49152\n",
      "  num bytes sharedMem Per Multiprocessor: 65536\n",
      "  Memory Clock Rate (KHz): 5001000\n",
      "  Memory Bus Width (bits): 256\n",
      "  Peak Memory Bandwidth (GB/s): 320.064000\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix common-convolution of 8192x512. block(2): 14.498272 ms.\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix common-convolution of 8192x512. block(4): 11.850944 ms.\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix common-convolution of 8192x512. block(8): 9.136864 ms.\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix common-convolution of 8192x512. block(16): 7.995168 ms.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix tiled-convolution of 8192x512. tile(2): 11.502624 ms.\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix tiled-convolution of 8192x512. tile(4): 9.692128 ms.\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix tiled-convolution of 8192x512. tile(8): 9.357760 ms.\n",
      "\n",
      "successs convolution; Time elapsed on naive GPU matrix tiled-convolution of 8192x512. tile(16): 7.460128 ms.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "#include <cuda.h>\n",
    "\n",
    "#define MATRIX_HEIGHT 8192\n",
    "#define MATRIX_WIDTH 512    // if matrix is too large, please close the verify which cost a lot of time\n",
    "#define BLOCK_START_DIM 2    // Start dim of block in for loop，limit 32 in T4\n",
    "#define MASK_SIZE 5          // convolution kernel size\n",
    "#define FILTER_RADIUS (MASK_SIZE/2)  // radius of convolution kernel\n",
    "\n",
    "// constant memory for mask\n",
    "__constant__ int mask_c[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1];\n",
    "\n",
    "// kernel of tiled 2D-convolution\n",
    "__global__ void convolution_tiled_2D_const_mem_kernel(int *input, int *mask ,int *output, int height, int width, int in_tile_dim, int out_tile_dim) {\n",
    "\n",
    "    int col = blockIdx.x * out_tile_dim + threadIdx.x - FILTER_RADIUS;\n",
    "    int row = blockIdx.y * out_tile_dim + threadIdx.y - FILTER_RADIUS;\n",
    "\n",
    "    // Loading input tile\n",
    "    extern __shared__ int sharedInput[];\n",
    "    if(row >= 0 && row < height && col >= 0 && col < width) {\n",
    "        sharedInput[threadIdx.y * in_tile_dim + threadIdx.x] = input[row * width + col];\n",
    "    } else {\n",
    "        sharedInput[threadIdx.y * in_tile_dim + threadIdx.x] = 0;\n",
    "    }\n",
    "\n",
    "    __syncthreads();\n",
    "\n",
    "    // Calculating output elements\n",
    "    int tileCol = threadIdx.x - FILTER_RADIUS;\n",
    "    int tileRow = threadIdx.y - FILTER_RADIUS;\n",
    "\n",
    "    // Turning off the threads at the edges of the block\n",
    "    if (col >= 0 && col < width && row >= 0 && row < height) {\n",
    "        if (tileCol >= 0 && tileCol < out_tile_dim && tileRow >= 0 && tileRow < out_tile_dim) {\n",
    "            float Pvalue = 0;\n",
    "            for (int fRow = 0; fRow < 2*FILTER_RADIUS+1; fRow++) {\n",
    "                for (int fCol = 0; fCol < 2*FILTER_RADIUS+1; fCol++) {\n",
    "                    Pvalue += mask_c[fRow][fCol] * sharedInput[(tileRow + fRow) * in_tile_dim + (tileCol + fCol)];\n",
    "                }\n",
    "            }\n",
    "            output[row * width + col] = Pvalue;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// kernel of 2D-convolution\n",
    "__global__ void gpu_matrix_convolute(int *input, int *mask, int *output, int height, int width)\n",
    "{\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if( col < width && row < height)\n",
    "    {\n",
    "      int pixVal = 0;\n",
    "      int start_col = col - (MASK_SIZE / 2);\n",
    "      int start_row = row - (MASK_SIZE / 2);\n",
    "\n",
    "      // Get the of the surrounding box\n",
    "      for(int i = 0; i < MASK_SIZE; ++i) {\n",
    "        for(int j = 0; j < MASK_SIZE; ++j) {\n",
    "          int cur_row = start_row + i;\n",
    "          int cur_col = start_col + j;\n",
    "\n",
    "          // Verify we have a valid image pixel\n",
    "          if(cur_row > -1 && cur_row < height && cur_col > -1 && cur_col < width) {\n",
    "            pixVal += input[cur_row * width + cur_col] * mask[i * MASK_SIZE + j];\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "      output[row * width + col] = pixVal;\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "// verify code implement convolution in serial\n",
    "void verify(int *input, int *mask, int *result, int height, int width){\n",
    "    int pixVal;\n",
    "    // Intermediate value for more readable code\n",
    "    int offset_r;\n",
    "    int offset_c;\n",
    "    // Go over each row\n",
    "    for(int i = 0;i < height; i++){\n",
    "        for(int j = 0; j < width; j++){\n",
    "            pixVal = 0;\n",
    "            for(int k = 0; k < MASK_SIZE; k++){\n",
    "                offset_r = i - MASK_SIZE / 2 + k;\n",
    "                for(int l = 0; l < MASK_SIZE; l++){\n",
    "                    offset_c = j - MASK_SIZE / 2 + l;\n",
    "                    if(offset_r >= 0 && offset_r < height){\n",
    "                        if(offset_c >= 0 && offset_c < width){\n",
    "                            pixVal += input[offset_r * width + offset_c] * mask[k * MASK_SIZE + l];\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            // Fail if the results don't match\n",
    "            if(result[i * width + j] != pixVal)\n",
    "            {\n",
    "                printf(\"fail convolution; \");\n",
    "                return;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    printf(\"successs convolution; \");\n",
    "    return;\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char const *argv[])\n",
    "{\n",
    "    // retrieve some info about the CUDA device\n",
    "    int nDevices;\n",
    "    cudaGetDeviceCount(&nDevices);\n",
    "    for (int i = 0; i < nDevices; i++) {\n",
    "      cudaDeviceProp prop;\n",
    "      cudaGetDeviceProperties(&prop, i);\n",
    "      printf(\"Device Number: %d\\n\", i);\n",
    "      printf(\"  Device name: %s\\n\", prop.name);\n",
    "      printf(\"  max Blocks Per MultiProcessor: %d\\n\", prop.maxBlocksPerMultiProcessor);\n",
    "      printf(\"  max Threads Per MultiProcessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
    "      printf(\"  max Threads Per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "      printf(\"  num SM: %d\\n\", prop.multiProcessorCount);\n",
    "      printf(\"  num bytes sharedMem Per Block: %d\\n\", prop.sharedMemPerBlock);\n",
    "      printf(\"  num bytes sharedMem Per Multiprocessor: %d\\n\", prop.sharedMemPerMultiprocessor);\n",
    "      printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
    "           prop.memoryClockRate);\n",
    "      printf(\"  Memory Bus Width (bits): %d\\n\",\n",
    "           prop.memoryBusWidth);\n",
    "      printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
    "           2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
    "    }\n",
    "\n",
    "\n",
    "    // execution\n",
    "    \n",
    "\n",
    "    \n",
    "    // execute common 2D-convolution\n",
    "    for(int block_size = BLOCK_START_DIM; block_size <= BLOCK_START_DIM * 8; (block_size *= 2))\n",
    "    {\n",
    "        // exception \n",
    "        if(block_size * block_size > 1024){\n",
    "            printf(\"The block size is excceed\\n\\n\");\n",
    "            continue;\n",
    "        }\n",
    "        int *input, *mask, *output;\n",
    "        cudaMallocManaged((void **) &input, sizeof(int)*MATRIX_HEIGHT*MATRIX_WIDTH);\n",
    "        cudaMallocManaged((void **) &mask, sizeof(int)*MASK_SIZE*MASK_SIZE);\n",
    "        cudaMallocManaged((void **) &output, sizeof(int)*MATRIX_HEIGHT*MATRIX_WIDTH);\n",
    "    \n",
    "        // initialize matrix A\n",
    "        for (int i = 0; i < MATRIX_HEIGHT; ++i) {\n",
    "            for (int j = 0; j < MATRIX_WIDTH; ++j) {\n",
    "                input[i * MATRIX_WIDTH + j] = 2;\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        // initialize matrix B\n",
    "        for (int i = 0; i < MASK_SIZE; ++i) {\n",
    "            for (int j = 0; j < MASK_SIZE; ++j) {\n",
    "                mask[i * MASK_SIZE + j] = i + j;\n",
    "            }\n",
    "        }\n",
    "        // copy mask kernel to constant memory\n",
    "        cudaMemcpyToSymbol(mask_c, mask, sizeof(int) * MASK_SIZE * MASK_SIZE); // 使用常量内存\n",
    "    \n",
    "        // some events to count the execution time\n",
    "        cudaEvent_t start, stop;\n",
    "        cudaEventCreate(&start);\n",
    "        cudaEventCreate(&stop);\n",
    "        float  naive_gpu_elapsed_time_ms;\n",
    "            \n",
    "        // define block and grid size\n",
    "        dim3 blockDim(block_size, block_size);\n",
    "        dim3 gridDim((MATRIX_WIDTH + block_size - 1) / block_size, (MATRIX_HEIGHT + block_size - 1) / block_size);\n",
    "        int *result = new int[MATRIX_HEIGHT * MATRIX_WIDTH];\n",
    "\n",
    "        //warm-up execution\n",
    "        gpu_matrix_convolute<<<gridDim, blockDim>>>(input, mask, output, MATRIX_HEIGHT, MATRIX_WIDTH);\n",
    "        \n",
    "        // time counting start\n",
    "        cudaEventRecord(start, 0);\n",
    "        gpu_matrix_convolute<<<gridDim, blockDim>>>(input, mask, output, MATRIX_HEIGHT, MATRIX_WIDTH);\n",
    "        cudaThreadSynchronize();\n",
    "    \n",
    "        // time counting terminate\n",
    "        cudaEventRecord(stop, 0);\n",
    "        cudaEventSynchronize(stop);\n",
    "    \n",
    "        //verify result\n",
    "        cudaMemcpy(result,output,MATRIX_HEIGHT * MATRIX_WIDTH * sizeof(int),cudaMemcpyDeviceToHost);\n",
    "        verify(input, mask, result, MATRIX_HEIGHT, MATRIX_WIDTH);\n",
    "    \n",
    "        // compute time elapsed on GPU computing\n",
    "        cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n",
    "        printf(\"Time elapsed on naive GPU matrix common-convolution of %dx%d. block(%d): %f ms.\\n\\n\", MATRIX_HEIGHT, MATRIX_WIDTH, block_size, naive_gpu_elapsed_time_ms);\n",
    "    \n",
    "        // free memory\n",
    "        free(result);\n",
    "        cudaFree(input);\n",
    "        cudaFree(mask);\n",
    "        cudaFree(output);\n",
    "    }\n",
    "\n",
    "    printf(\"----------------------------------------------------------------------------------------------------------------\\n\\n\");\n",
    "\n",
    "    // execute tiled 2D-convolution\n",
    "    for(int out_tile_dim = BLOCK_START_DIM; out_tile_dim <= BLOCK_START_DIM * 8; (out_tile_dim *= 2))\n",
    "    {\n",
    "        int in_tile_dim = out_tile_dim + 2 * FILTER_RADIUS;\n",
    "\n",
    "        // exception \n",
    "        if(out_tile_dim * out_tile_dim > 1024){\n",
    "            printf(\"The out_tile size is excceed\\n\\n\");\n",
    "            continue;\n",
    "        }else if(in_tile_dim * in_tile_dim > 1024){\n",
    "            printf(\"The in_tile size is excceed\\n\\n\");\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "        int *input, *mask, *output;\n",
    "        cudaMallocManaged((void **) &input, sizeof(int)*MATRIX_HEIGHT*MATRIX_WIDTH);\n",
    "        cudaMallocManaged((void **) &mask, sizeof(int)*MASK_SIZE*MASK_SIZE);\n",
    "        cudaMallocManaged((void **) &output, sizeof(int)*MATRIX_HEIGHT*MATRIX_WIDTH);\n",
    "    \n",
    "        // initialize matrix A\n",
    "        for (int i = 0; i < MATRIX_HEIGHT; ++i) {\n",
    "            for (int j = 0; j < MATRIX_WIDTH; ++j) {\n",
    "                input[i * MATRIX_WIDTH + j] = 2;\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        // initialize matrix B\n",
    "        for (int i = 0; i < MASK_SIZE; ++i) {\n",
    "            for (int j = 0; j < MASK_SIZE; ++j) {\n",
    "                mask[i * MASK_SIZE + j] = i + j;\n",
    "            }\n",
    "        }\n",
    "        // copy mask kernel to constant memory\n",
    "        cudaMemcpyToSymbol(mask_c, mask, sizeof(int) * MASK_SIZE * MASK_SIZE); // 使用常量内存\n",
    "    \n",
    "        // some events to count the execution time\n",
    "        cudaEvent_t start, stop;\n",
    "        cudaEventCreate(&start);\n",
    "        cudaEventCreate(&stop);\n",
    "        float  naive_gpu_elapsed_time_ms;\n",
    "    \n",
    "        // define block and grid size\n",
    "        dim3 blockDim(in_tile_dim, in_tile_dim);\n",
    "        dim3 gridDim((MATRIX_WIDTH + out_tile_dim - 1) / out_tile_dim, (MATRIX_HEIGHT + out_tile_dim - 1) / out_tile_dim);\n",
    "        size_t sharedMemorySize = in_tile_dim * in_tile_dim * sizeof(int);\n",
    "        int *result = new int[MATRIX_HEIGHT * MATRIX_WIDTH];\n",
    "    \n",
    "        //warm-up execution\n",
    "        convolution_tiled_2D_const_mem_kernel<<<gridDim, blockDim, sharedMemorySize>>>(input, mask, output, MATRIX_HEIGHT, MATRIX_WIDTH, in_tile_dim, out_tile_dim);\n",
    "\n",
    "\n",
    "        // time counting start\n",
    "        cudaEventRecord(start, 0);\n",
    "        convolution_tiled_2D_const_mem_kernel<<<gridDim, blockDim, sharedMemorySize>>>(input, mask, output, MATRIX_HEIGHT, MATRIX_WIDTH, in_tile_dim, out_tile_dim);\n",
    "        cudaThreadSynchronize();\n",
    "    \n",
    "        // time counting terminate\n",
    "        cudaEventRecord(stop, 0);\n",
    "        cudaEventSynchronize(stop);\n",
    "    \n",
    "        //verify result\n",
    "        cudaMemcpy(result,output,MATRIX_HEIGHT * MATRIX_WIDTH * sizeof(int),cudaMemcpyDeviceToHost);\n",
    "        verify(input, mask, result, MATRIX_HEIGHT, MATRIX_WIDTH);\n",
    "    \n",
    "        // compute time elapsed on GPU computing\n",
    "        cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n",
    "        printf(\"Time elapsed on naive GPU matrix tiled-convolution of %dx%d. tile(%d): %f ms.\\n\\n\", MATRIX_HEIGHT, MATRIX_WIDTH, out_tile_dim, naive_gpu_elapsed_time_ms);\n",
    "    \n",
    "        // free memory\n",
    "        free(result);\n",
    "        cudaFree(input);\n",
    "        cudaFree(mask);\n",
    "        cudaFree(output);\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ctsef02Z2aYI",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oDfyrVUoi9p0",
    "aO9tJ69Qo7uM",
    "xqxuVMTxpRtq",
    "IXgu6mWGz7Bg"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
