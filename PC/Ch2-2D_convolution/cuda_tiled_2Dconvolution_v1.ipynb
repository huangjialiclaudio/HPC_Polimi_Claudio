{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["oDfyrVUoi9p0","aO9tJ69Qo7uM","xqxuVMTxpRtq","IXgu6mWGz7Bg"]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvcc --version","metadata":{"id":"nNS2FA5BR0bU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd3b83c4-09e3-4336-d9ba-7c2f16e59a9c","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:22:12.252720Z","iopub.execute_input":"2024-12-10T07:22:12.253076Z","iopub.status.idle":"2024-12-10T07:22:13.305687Z","shell.execute_reply.started":"2024-12-10T07:22:12.253047Z","shell.execute_reply":"2024-12-10T07:22:13.304636Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Wed_Nov_22_10:17:15_PST_2023\nCuda compilation tools, release 12.3, V12.3.107\nBuild cuda_12.3.r12.3/compiler.33567101_0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Initialize a nvcc plugin for python notebook","metadata":{"id":"n6hCYQF3T2f7"}},{"cell_type":"code","source":"!pip install nvcc4jupyter","metadata":{"id":"e1MqBxDxUBTo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1aa2e545-1665-46d8-bd82-aa928d917ff2","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:22:13.308292Z","iopub.execute_input":"2024-12-10T07:22:13.308992Z","iopub.status.idle":"2024-12-10T07:22:22.753306Z","shell.execute_reply.started":"2024-12-10T07:22:13.308949Z","shell.execute_reply":"2024-12-10T07:22:22.752449Z"}},"outputs":[{"name":"stdout","text":"Collecting nvcc4jupyter\n  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\nDownloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\nInstalling collected packages: nvcc4jupyter\nSuccessfully installed nvcc4jupyter-1.2.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Load the plugin extension","metadata":{"id":"dlUTHXk-UM0o"}},{"cell_type":"code","source":"%load_ext nvcc4jupyter","metadata":{"id":"V23O5ZJFUQn4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"824c4fc6-02fa-4e6f-f07f-62cada73033b","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T07:22:22.754433Z","iopub.execute_input":"2024-12-10T07:22:22.754713Z","iopub.status.idle":"2024-12-10T07:24:54.940111Z","shell.execute_reply.started":"2024-12-10T07:22:22.754687Z","shell.execute_reply":"2024-12-10T07:24:54.939195Z"}},"outputs":[{"name":"stdout","text":"Detected platform \"Kaggle\". Running its setup...\nUpdating the package lists...\nInstalling nvidia-cuda-toolkit, this may take a few minutes...\nSource files will be saved in \"/tmp/tmpfghygz6i\".\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{"id":"K6PwQnXhCBnH"}},{"cell_type":"code","source":"%%cuda\n#include <stdio.h>\n#include <stdlib.h>\n#include <time.h>\n#include <cuda.h>\n\n#define MATRIX_HEIGHT 8192\n#define MATRIX_WIDTH 512\n#define TILE_DIM 16\n#define MASK_SIZE 5\n#define FILTER_RADIUS (MASK_SIZE/2)\n#define IN_TILE_DIM TILE_DIM\n#define OUT_TILE_DIM (IN_TILE_DIM - 2 * (FILTER_RADIUS))\n\n__constant__ int mask_c[2*FILTER_RADIUS+1][2*FILTER_RADIUS+1];\n\n__global__ void convolution_tiled_2D_const_mem_kernel(int *input, int *mask ,int *output, int height, int width) {\n\n    int col = blockIdx.x * OUT_TILE_DIM + threadIdx.x - FILTER_RADIUS;\n    int row = blockIdx.y * OUT_TILE_DIM + threadIdx.y - FILTER_RADIUS;\n\n    // Loading input tile\n    __shared__ int sharedInput[IN_TILE_DIM][IN_TILE_DIM];\n    if(row >= 0 && row < height && col >= 0 && col < width) {\n        sharedInput[threadIdx.y][threadIdx.x] = input[row*width + col];\n    } else {\n        sharedInput[threadIdx.y][threadIdx.x] = 0;\n    }\n\n    __syncthreads();\n\n    // Calculating output elements\n    int tileCol = threadIdx.x - FILTER_RADIUS;\n    int tileRow = threadIdx.y - FILTER_RADIUS;\n\n    // Turning off the threads at the edges of the block\n    if (col >= 0 && col < width && row >= 0 && row < height) {\n        if (tileCol >= 0 && tileCol < OUT_TILE_DIM && tileRow >= 0 && tileRow < OUT_TILE_DIM) {\n            float Pvalue = 0;\n            for (int fRow = 0; fRow < 2*FILTER_RADIUS+1; fRow++) {\n                for (int fCol = 0; fCol < 2*FILTER_RADIUS+1; fCol++) {\n                    Pvalue += mask_c[fRow][fCol] * sharedInput[tileRow + fRow][tileCol + fCol];\n                }\n            }\n            output[row * width + col] = Pvalue;\n        }\n    }\n}\n\n\n__global__ void gpu_matrix_convolute(int *input, int *mask, int *output, int height, int width)\n{\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if( col < width && row < height)\n    {\n      int pixVal = 0;\n      int start_col = col - (MASK_SIZE / 2);\n      int start_row = row - (MASK_SIZE / 2);\n\n      // Get the of the surrounding box\n      for(int i = 0; i < MASK_SIZE; ++i) {\n        for(int j = 0; j < MASK_SIZE; ++j) {\n          int cur_row = start_row + i;\n          int cur_col = start_col + j;\n\n          // Verify we have a valid image pixel\n          if(cur_row > -1 && cur_row < height && cur_col > -1 && cur_col < width) {\n            pixVal += input[cur_row * width + cur_col] * mask[i * MASK_SIZE + j];\n          }\n        }\n      }\n      output[row * width + col] = pixVal;\n    }\n}\n\n\n\nvoid verify(int *input, int *mask, int *result, int height, int width){\n    int pixVal;\n    // Intermediate value for more readable code\n    int offset_r;\n    int offset_c;\n    // Go over each row\n    for(int i = 0;i < height; i++){\n        for(int j = 0; j < width; j++){\n            pixVal = 0;\n            for(int k = 0; k < MASK_SIZE; k++){\n                offset_r = i - MASK_SIZE / 2 + k;\n                for(int l = 0; l < MASK_SIZE; l++){\n                    offset_c = j - MASK_SIZE / 2 + l;\n                    if(offset_r >= 0 && offset_r < height){\n                        if(offset_c >= 0 && offset_c < width){\n                            pixVal += input[offset_r * width + offset_c] * mask[k * MASK_SIZE + l];\n                        }\n                    }\n                }\n            }\n            // Fail if the results don't match\n            if(result[i * width + j] != pixVal)\n            {\n                printf(\"fail convolution; \");\n                return;\n            }\n        }\n    }\n    printf(\"successs convolution; \");\n    return;\n}\n\n\nint main(int argc, char const *argv[])\n{\n    // retrieve some info about the CUDA device\n    int nDevices;\n    cudaGetDeviceCount(&nDevices);\n    for (int i = 0; i < nDevices; i++) {\n      cudaDeviceProp prop;\n      cudaGetDeviceProperties(&prop, i);\n      printf(\"Device Number: %d\\n\", i);\n      printf(\"  Device name: %s\\n\", prop.name);\n      printf(\"  max Blocks Per MultiProcessor: %d\\n\", prop.maxBlocksPerMultiProcessor);\n      printf(\"  max Threads Per MultiProcessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n      printf(\"  max Threads Per Block: %d\\n\", prop.maxThreadsPerBlock);\n      printf(\"  num SM: %d\\n\", prop.multiProcessorCount);\n      printf(\"  num bytes sharedMem Per Block: %d\\n\", prop.sharedMemPerBlock);\n      printf(\"  num bytes sharedMem Per Multiprocessor: %d\\n\", prop.sharedMemPerMultiprocessor);\n      printf(\"  Memory Clock Rate (KHz): %d\\n\",\n           prop.memoryClockRate);\n      printf(\"  Memory Bus Width (bits): %d\\n\",\n           prop.memoryBusWidth);\n      printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n           2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n    }\n\n\n    // execution\n    int *input, *mask, *output;\n    cudaMallocManaged((void **) &input, sizeof(int)*MATRIX_HEIGHT*MATRIX_WIDTH);\n    cudaMallocManaged((void **) &mask, sizeof(int)*MASK_SIZE*MASK_SIZE);\n    cudaMallocManaged((void **) &output, sizeof(int)*MATRIX_HEIGHT*MATRIX_WIDTH);\n    int *result = new int[MATRIX_HEIGHT * MATRIX_WIDTH];\n    // initialize matrix A\n    for (int i = 0; i < MATRIX_HEIGHT; ++i) {\n        for (int j = 0; j < MATRIX_WIDTH; ++j) {\n            input[i * MATRIX_WIDTH + j] = 2;\n        }\n    }\n\n    // initialize matrix B\n    for (int i = 0; i < MASK_SIZE; ++i) {\n        for (int j = 0; j < MASK_SIZE; ++j) {\n            mask[i * MASK_SIZE + j] = i + j;\n        }\n    }\n    // copy mask kernel to constant memory\n    cudaMemcpyToSymbol(mask_c, mask, sizeof(int) * MASK_SIZE * MASK_SIZE); // 使用常量内存\n\n    // some events to count the execution time\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n    float  naive_gpu_elapsed_time_ms;\n\n        \n    // execute common 2D-convolution\n    {\n    // define block and grid size\n    dim3 blockDim(OUT_TILE_DIM, OUT_TILE_DIM);\n    dim3 gridDim((MATRIX_WIDTH + OUT_TILE_DIM - 1) / OUT_TILE_DIM, (MATRIX_HEIGHT + OUT_TILE_DIM - 1) / OUT_TILE_DIM);\n\n    // time counting start\n    cudaEventRecord(start, 0);\n    gpu_matrix_convolute<<<gridDim, blockDim>>>(input, mask, output, MATRIX_HEIGHT, MATRIX_WIDTH);\n    cudaThreadSynchronize();\n\n    // time counting terminate\n    cudaEventRecord(stop, 0);\n    cudaEventSynchronize(stop);\n\n    //verify result\n    cudaMemcpy(result,output,MATRIX_HEIGHT * MATRIX_WIDTH * sizeof(int),cudaMemcpyDeviceToHost);\n    verify(input, mask, result, MATRIX_HEIGHT, MATRIX_WIDTH);\n\n    // compute time elapsed on GPU computing\n    cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n    printf(\"Time elapsed on naive GPU matrix common-convolution of %dx%d. mask(%d): %f ms.\\n\\n\", MATRIX_HEIGHT, MATRIX_WIDTH, MASK_SIZE, naive_gpu_elapsed_time_ms);\n    }\n\n\n    // execute tiled 2D-convolution\n    {\n    // define block and grid size\n    dim3 blockDim(IN_TILE_DIM, IN_TILE_DIM);\n    dim3 gridDim((MATRIX_WIDTH + OUT_TILE_DIM - 1) / OUT_TILE_DIM, (MATRIX_HEIGHT + OUT_TILE_DIM - 1) / OUT_TILE_DIM);\n\n    // time counting start\n    cudaEventRecord(start, 0);\n    convolution_tiled_2D_const_mem_kernel<<<gridDim, blockDim>>>(input, mask, output, MATRIX_HEIGHT, MATRIX_WIDTH);\n    cudaThreadSynchronize();\n\n    // time counting terminate\n    cudaEventRecord(stop, 0);\n    cudaEventSynchronize(stop);\n\n    //verify result\n    cudaMemcpy(result,output,MATRIX_HEIGHT * MATRIX_WIDTH * sizeof(int),cudaMemcpyDeviceToHost);\n    verify(input, mask, result, MATRIX_HEIGHT, MATRIX_WIDTH);\n\n    // compute time elapsed on GPU computing\n    cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n    printf(\"Time elapsed on naive GPU matrix tiled-convolution of %dx%d. mask(%d): %f ms.\\n\\n\", MATRIX_HEIGHT, MATRIX_WIDTH, MASK_SIZE, naive_gpu_elapsed_time_ms);\n    }\n\n    \n\n\n\n    // free memory\n    cudaFree(input);\n    cudaFree(mask);\n    cudaFree(output);\n\n    return 0;\n}\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfHv2_VUCBXH","outputId":"cd7f6766-779d-49d6-e7ef-93a41dc5d02c","trusted":true,"execution":{"iopub.status.busy":"2024-12-10T08:00:10.424408Z","iopub.execute_input":"2024-12-10T08:00:10.424878Z","iopub.status.idle":"2024-12-10T08:00:12.927690Z","shell.execute_reply.started":"2024-12-10T08:00:10.424831Z","shell.execute_reply":"2024-12-10T08:00:12.926792Z"}},"outputs":[{"name":"stdout","text":"Device Number: 0\n  Device name: Tesla T4\n  max Blocks Per MultiProcessor: 16\n  max Threads Per MultiProcessor: 1024\n  max Threads Per Block: 1024\n  num SM: 40\n  num bytes sharedMem Per Block: 49152\n  num bytes sharedMem Per Multiprocessor: 65536\n  Memory Clock Rate (KHz): 5001000\n  Memory Bus Width (bits): 256\n  Peak Memory Bandwidth (GB/s): 320.064000\n\nDevice Number: 1\n  Device name: Tesla T4\n  max Blocks Per MultiProcessor: 16\n  max Threads Per MultiProcessor: 1024\n  max Threads Per Block: 1024\n  num SM: 40\n  num bytes sharedMem Per Block: 49152\n  num bytes sharedMem Per Multiprocessor: 65536\n  Memory Clock Rate (KHz): 5001000\n  Memory Bus Width (bits): 256\n  Peak Memory Bandwidth (GB/s): 320.064000\n\nsuccesss convolution; Time elapsed on naive GPU matrix common-convolution of 8192x512. mask(5): 8.780160 ms.\n\nsuccesss convolution; Time elapsed on naive GPU matrix tiled-convolution of 8192x512. mask(5): 5.275424 ms.\n\n\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"Ctsef02Z2aYI"},"outputs":[],"execution_count":null}]}